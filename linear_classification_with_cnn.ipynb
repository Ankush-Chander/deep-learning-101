{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_classification_with_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO9w8RODGEDdq5DX5Y7x4El",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankush-Chander/deep-learning-101/blob/main/linear_classification_with_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI5YtYsTp0Zd",
        "outputId": "617745a2-c46a-4c25-da46-529e0fb89a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gl8GQfJIGft",
        "outputId": "0747a518-661b-4dbf-c328-eca3b05e7704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# load dataset \n",
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "num_words=10000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize dataset inputs\n",
        "# 1. load glove word2vec\n",
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "\n",
        "# vectorize dataset input\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {val:key for key, val in word_index.items()}\n",
        "\n",
        "# print(\" \".join([reverse_word_index.get(i-3, \"?\") for i in train_data[0]]))\n",
        "def tokenize_and_vectorize(train_data):\n",
        "  vectorized_train_data = []\n",
        "  for sample in train_data:\n",
        "    sample_vector = []\n",
        "    for token_id in sample:\n",
        "      try:\n",
        "        word = reverse_word_index.get(token_id-3, \"?\")\n",
        "        word_vector = word_vectors[word]\n",
        "        sample_vector.append(word_vector)\n",
        "      except KeyError as err:\n",
        "        pass\n",
        "    vectorized_train_data.append(sample_vector)\n",
        "\n",
        "  return vectorized_train_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbx4j-V3JpZY",
        "outputId": "1990c0b5-5808-4fa6-8a09-ab82ac305aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_data = tokenize_and_vectorize(train_data)\n",
        "expected = train_labels"
      ],
      "metadata": {
        "id": "3P0Ww__eQe4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test/train split\n",
        "split_point = int(len(vectorized_data)*.8)\n",
        "x_train = vectorized_data[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_train = expected[:split_point]\n",
        "y_test =  expected[split_point:]\n",
        "len(x_train[1])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J19xeIwrQzZJ",
        "outputId": "744c2375-085c-45a7-b32c-5c7a1901d463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "185"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN parameters\n",
        "maxlen = 400 # maximum length of sentences to be considered for sentiment analysis\n",
        "batch_size = 32 # How many samples to show to the neural net before backpropagation\n",
        "embedding_dims = 100 # length of token embedding vector\n",
        "filters = 250 # Number of filters you will train\n",
        "kernel_size = 3 # Width of filter\n",
        "hidden_dims = 250 # Number of neurons at the end of plain feedforward net at the end if the chain\n",
        "epochs = 2 # Number of times you will pass the entire dataset through the network\n"
      ],
      "metadata": {
        "id": "lVAkiEIPjFp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding and truncating your token sequence\n",
        "import numpy as np\n",
        "print(min([len(sample) for sample in x_train]))\n",
        "print(max([len(sample) for sample in x_train]))\n",
        "print(len(x_train[0][0]))\n",
        "\n",
        "def pad_trunc(dataset, maxlen, embedding_dims):\n",
        "  return [sample[:maxlen] + (maxlen - len(sample)) * [[0.]*embedding_dims] for sample in dataset]\n",
        "\n",
        "x_train = pad_trunc(x_train, maxlen, embedding_dims)\n",
        "x_test = pad_trunc(x_test, maxlen, embedding_dims)\n",
        "\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BVzmreuVShxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1927ad-bea7-4cc6-c8fb-0b958c319185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "2477\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a 1D CNN\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding=\"valid\",activation=\"relu\", input_shape=(maxlen, embedding_dims)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(\"sigmoid\"))\n"
      ],
      "metadata": {
        "id": "eu5dWPclSj0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the CNN\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "QiZ_t-TXsYvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your model\n",
        "model.fit(x_train, y_train,  batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "5WtH9YHZSuz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48ccb83-cf61-4bca-c4e8-2a4c18e7283e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "625/625 [==============================] - 44s 69ms/step - loss: 0.4390 - accuracy: 0.7861 - val_loss: 0.4217 - val_accuracy: 0.8104\n",
            "Epoch 2/2\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.2970 - accuracy: 0.8733 - val_loss: 0.3251 - val_accuracy: 0.8646\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9b7ba36b90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your model\n",
        "model_structure = model.to_json()\n",
        "with open(\"cnn_model.json\", \"w\") as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights(\"cnn_weights.h5\")"
      ],
      "metadata": {
        "id": "FkcruO_otMfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model in the pipeline\n",
        "# load model\n",
        "\n",
        "from keras.models import model_from_json\n",
        "with open(\"cnn_model.json\", \"r\") as json_file:\n",
        "  json_string = json_file.read()\n",
        "\n",
        "loaded_model  = model_from_json(json_string)\n",
        "loaded_model.load_weights('cnn_weights.h5')\n",
        "\n",
        "negative_sample = \"I didnt like the movie at all.\"\n",
        "positive_sample = \"The movie was awesome\"\n",
        "ambigous_sample1 = \"The acting was awesome but plot was horrible.\"\n",
        "ambigous_sample2 = \"The movie was okayish.\"\n",
        "\n",
        "def vectorize_input_text(dataset):\n",
        "  vectorized_dataset = []\n",
        "  for text in dataset:\n",
        "    sample_vector = []\n",
        "    for word in text.split():\n",
        "      try:\n",
        "        word_vector = word_vectors[word]\n",
        "        sample_vector.append(word_vector)\n",
        "      except KeyError as err:\n",
        "        pass\n",
        "    vectorized_dataset.append(sample_vector)\n",
        "  return vectorized_dataset  \n",
        "\n",
        "\n",
        "def predict_sentiment(input_data:list):\n",
        "  # convert input_data into vectorized format\n",
        "  vectorized_input_data = vectorize_input_text(input_data)\n",
        "  # padding_truncating\n",
        "  trunc_data = pad_trunc(vectorized_input_data, maxlen=maxlen, embedding_dims=embedding_dims)\n",
        "  # reshape data\n",
        "  reshaped_input_data = np.reshape(trunc_data, (len(trunc_data), maxlen, embedding_dims))\n",
        "  x = loaded_model.predict(reshaped_input_data)\n",
        "  return x\n",
        "\n",
        "x = predict_sentiment([negative_sample, positive_sample, ambigous_sample1, ambigous_sample2])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnXcdcLitjN3",
        "outputId": "d1710c3d-bb82-4dec-d320-1e872039a8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.25989667]\n",
            " [0.87162495]\n",
            " [0.5543196 ]\n",
            " [0.2896278 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T15JJ9xozWXD",
        "outputId": "dfe48c35-0a2d-4156-f64f-7892d66ea895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 398, 250)          75250     \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 250)              0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 250)               62750     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 250)               0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 250)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 251       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,251\n",
            "Trainable params: 138,251\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}